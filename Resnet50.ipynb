{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Resnet50.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavipatil29/Emotion-Detection-/blob/main/Resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IEp61vreDGfo",
        "outputId": "3161eb15-cb8f-4000-bf6e-505613ab21e1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import skimage.io\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Input, Flatten, Dense, Activation, Conv2D, MaxPool2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_vggface in /opt/conda/lib/python3.7/site-packages (0.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.5.4)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (5.3.1)\n",
            "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (2.10.0)\n",
            "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (7.2.0)\n",
            "Requirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (2.4.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.19.5)\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.5 MB/s eta 0:00:011\n",
            "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications) (1.15.0)\n",
            "Installing collected packages: keras-applications\n",
            "Successfully installed keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "s1-1Tl0WDGf0"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   validation_split = 0.2,\n",
        "                                  \n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        #zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  validation_split = 0.2)\n",
        "\n",
        "test_datagen  = ImageDataGenerator(rescale = 1./255\n",
        "                                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jS4VGhf7DGf1",
        "outputId": "2de70475-afed-4c30-e30a-dcea86f577b4"
      },
      "source": [
        "train_dataset  = train_datagen.flow_from_directory(directory = '../input/fer2013/train',\n",
        "                                                   target_size = (48,48),\n",
        "                                                   class_mode = 'categorical',\n",
        "                                                   subset = 'training',\n",
        "                                                   batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22968 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lOJrAC5qDGf2",
        "outputId": "80358851-5eae-499e-bfc6-581bcf4311c9"
      },
      "source": [
        "valid_dataset = valid_datagen.flow_from_directory(directory = '../input/fer2013/train',\n",
        "                                                  target_size = (48,48),\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  subset = 'validation',\n",
        "                                                  batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5741 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HkM7f2xODGf2",
        "outputId": "bc3f7be4-8b90-488e-a9ad-208a6550b66f"
      },
      "source": [
        "test_dataset = test_datagen.flow_from_directory(directory = '../input/fer2013/test',\n",
        "                                                  target_size = (48,48),\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7178 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OwZsvPDTDGf3",
        "outputId": "385b5335-db08-4106-8e58-d0fdab5deda2"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "img = image.load_img(\"../input/fer2013/test/angry/PrivateTest_10131363.jpg\",target_size=(48,48))\n",
        "img = np.array(img)\n",
        "plt.imshow(img)\n",
        "print(img.shape)\n",
        "\n",
        "img = np.expand_dims(img, axis=0)\n",
        "from keras.models import load_model\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48, 48, 3)\n",
            "(1, 48, 48, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hwqNqpvEDGf4",
        "outputId": "bf01f415-59cb-42f3-fcb9-f3057a5db8ce"
      },
      "source": [
        "base_model = tf.keras.applications.ResNet50(input_shape=(48,48,3),include_top=False,weights=\"imagenet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Y8rIZXDGf5"
      },
      "source": [
        "![image.png](attachment:3dbc5d0f-9e33-48c9-9569-eb402e036002.png)![image.png](attachment:7d00b935-9bb2-4fb3-8ad0-eff47d9aab1a.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WkOjcL5fDGf5"
      },
      "source": [
        "# Freezing Layers\n",
        "\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7PHAwol0DGf5"
      },
      "source": [
        "# Building Model\n",
        "\n",
        "model=Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1094,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qQ_3tcu4DGf6",
        "outputId": "e3df46ae-f2c8-44d5-b4f3-dde96591f442"
      },
      "source": [
        "# Model Summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2, 2, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              33558528  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1094)              4482118   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1094)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 7665      \n",
            "=================================================================\n",
            "Total params: 61,636,023\n",
            "Trainable params: 39,103,031\n",
            "Non-trainable params: 22,532,992\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b220z4wIDGf6"
      },
      "source": [
        "def f1_score(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V0MCzVXODGf7"
      },
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),  \n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "        f1_score,\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xcAfxxKYDGf7"
      },
      "source": [
        "lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)\n",
        "\n",
        "mcp = ModelCheckpoint('model.h5')\n",
        "\n",
        "es = EarlyStopping(verbose=1, patience=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "m2x1XGSRDGf7"
      },
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F6f6xVW8DGf7",
        "outputId": "c981e81f-a075-49cb-a216-69830015187a"
      },
      "source": [
        "history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "359/359 [==============================] - 152s 406ms/step - loss: 2.7562 - accuracy: 0.8459 - precision: 0.1756 - recall: 0.0210 - auc: 0.6062 - f1_score: 0.0253 - val_loss: 1.8128 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6477 - val_f1_score: 0.0000e+00\n",
            "Epoch 2/5\n",
            "359/359 [==============================] - 40s 112ms/step - loss: 1.8309 - accuracy: 0.8571 - precision: 0.3310 - recall: 3.0243e-04 - auc: 0.6330 - f1_score: 5.9535e-04 - val_loss: 1.8069 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6489 - val_f1_score: 0.0000e+00\n",
            "Epoch 3/5\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8268 - accuracy: 0.8571 - precision: 0.1402 - recall: 9.6500e-05 - auc: 0.6370 - f1_score: 1.8934e-04 - val_loss: 1.8075 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6537 - val_f1_score: 0.0000e+00\n",
            "Epoch 4/5\n",
            "359/359 [==============================] - 39s 110ms/step - loss: 1.8245 - accuracy: 0.8571 - precision: 0.3263 - recall: 1.7419e-04 - auc: 0.6375 - f1_score: 3.4030e-04 - val_loss: 1.8039 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6542 - val_f1_score: 0.0000e+00\n",
            "Epoch 5/5\n",
            "359/359 [==============================] - 39s 110ms/step - loss: 1.8199 - accuracy: 0.8571 - precision: 0.0211 - recall: 4.8367e-06 - auc: 0.6407 - f1_score: 9.3768e-06 - val_loss: 1.8071 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6600 - val_f1_score: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l1T3_mv9DGf8",
        "outputId": "b876ca9c-f2d5-4b5f-d6fe-5b6e6d22240d"
      },
      "source": [
        "history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 10,verbose = 1,callbacks=[lrd,mcp,es])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "359/359 [==============================] - 39s 110ms/step - loss: 1.8187 - accuracy: 0.8571 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6428 - f1_score: 0.0000e+00 - val_loss: 1.8069 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6610 - val_f1_score: 0.0000e+00\n",
            "Epoch 2/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8170 - accuracy: 0.8571 - precision: 0.1000 - recall: 4.3539e-05 - auc: 0.6447 - f1_score: 8.5708e-05 - val_loss: 1.8044 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6546 - val_f1_score: 0.0000e+00\n",
            "Epoch 3/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.8168 - accuracy: 0.8571 - precision: 0.3333 - recall: 4.3539e-05 - auc: 0.6454 - f1_score: 8.5708e-05 - val_loss: 1.7879 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6734 - val_f1_score: 0.0000e+00\n",
            "Epoch 4/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8103 - accuracy: 0.8571 - precision: 0.4667 - recall: 3.0477e-04 - auc: 0.6489 - f1_score: 5.9996e-04 - val_loss: 1.7862 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6684 - val_f1_score: 0.0000e+00\n",
            "Epoch 5/10\n",
            "359/359 [==============================] - 39s 110ms/step - loss: 1.8094 - accuracy: 0.8571 - precision: 0.3750 - recall: 1.3062e-04 - auc: 0.6492 - f1_score: 2.5583e-04 - val_loss: 1.7931 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6693 - val_f1_score: 0.0000e+00\n",
            "Epoch 6/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8099 - accuracy: 0.8571 - precision: 0.3333 - recall: 1.3062e-04 - auc: 0.6496 - f1_score: 2.5712e-04 - val_loss: 1.7925 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6634 - val_f1_score: 0.0000e+00\n",
            "Epoch 7/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8083 - accuracy: 0.8571 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6500 - f1_score: 0.0000e+00 - val_loss: 1.7919 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6707 - val_f1_score: 0.0000e+00\n",
            "Epoch 8/10\n",
            "359/359 [==============================] - 40s 110ms/step - loss: 1.8074 - accuracy: 0.8571 - precision: 0.1500 - recall: 1.3062e-04 - auc: 0.6500 - f1_score: 2.5712e-04 - val_loss: 1.7908 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6692 - val_f1_score: 0.0000e+00\n",
            "Epoch 9/10\n",
            "359/359 [==============================] - 40s 110ms/step - loss: 1.8073 - accuracy: 0.8571 - precision: 0.0769 - recall: 4.3539e-05 - auc: 0.6502 - f1_score: 8.5708e-05 - val_loss: 1.7938 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6697 - val_f1_score: 0.0000e+00\n",
            "Epoch 10/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 1.8057 - accuracy: 0.8571 - precision: 0.2857 - recall: 8.7078e-05 - auc: 0.6516 - f1_score: 1.7142e-04 - val_loss: 1.7851 - val_accuracy: 0.8571 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6734 - val_f1_score: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QsiX1wKLDGf8"
      },
      "source": [
        "#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n",
        "\n",
        "def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n",
        "    \n",
        "    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n",
        "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
        "\n",
        "    ax1.plot(range(1, len(acc) + 1), acc)\n",
        "    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n",
        "    ax1.set_title('History of Accuracy')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['training', 'validation'])\n",
        "\n",
        "\n",
        "    ax2.plot(range(1, len(loss) + 1), loss)\n",
        "    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n",
        "    ax2.set_title('History of Loss')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['training', 'validation'])\n",
        "    \n",
        "    ax3.plot(range(1, len(auc) + 1), auc)\n",
        "    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n",
        "    ax3.set_title('History of AUC')\n",
        "    ax3.set_xlabel('Epochs')\n",
        "    ax3.set_ylabel('AUC')\n",
        "    ax3.legend(['training', 'validation'])\n",
        "    \n",
        "    ax4.plot(range(1, len(precision) + 1), precision)\n",
        "    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n",
        "    ax4.set_title('History of Precision')\n",
        "    ax4.set_xlabel('Epochs')\n",
        "    ax4.set_ylabel('Precision')\n",
        "    ax4.legend(['training', 'validation'])\n",
        "    \n",
        "    ax5.plot(range(1, len(f1) + 1), f1)\n",
        "    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n",
        "    ax5.set_title('History of F1-score')\n",
        "    ax5.set_xlabel('Epochs')\n",
        "    ax5.set_ylabel('F1 score')\n",
        "    ax5.legend(['training', 'validation'])\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n",
        "               history.history['loss'],history.history['val_loss'],\n",
        "               history.history['auc'],history.history['val_auc'],\n",
        "               history.history['precision'],history.history['val_precision'],\n",
        "               history.history['f1_score'],history.history['val_f1_score']\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}